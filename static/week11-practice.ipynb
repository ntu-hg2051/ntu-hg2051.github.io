{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "\n",
    "This week covers supervised classification.\n",
    "\n",
    "Overview:\n",
    "\n",
    "* [**Supervised Classification**](#Supervised-Classification)\n",
    "* [**Evaluation**](#Evaluation)\n",
    "* [**Decision Trees**](#Decision-Trees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "You can get a program to make decisions and act intelligently based on its inputs using programming constructs like conditional expressions, loops, etc., but it is often infeasible to construct programs for large, complicated, or ambiguous tasks. Machine Learning (ML) is how you can get computers to respond to inputs without hard-coded rules by learning from examples. The programmer's job is thus to teach the program how to learn from examples it is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "\n",
    "Supervised classification is a kind of machine learning where examples contain the correct labels that you want the system to predict. The system then extracts features from the examples and builds a model that helps it infer the correct label given the features. Then, when encountering a new, unlabeled instance, it guesses the likely label based on the instance's feature similarity to other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Given an instance, you need to produce features that the system can learn from. Good features are highly predictive of certain labels while excluding the other labels. Sometimes simple features work well, such as \"word ends in `ed`\", and other times complex features are more distinguishing, such as \"word ends in `ed` and previous word is tagged `MD`\".\n",
    "\n",
    "Write a feature extraction function for classifying names as male or female:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    feats = {}\n",
    "    return feats\n",
    "\n",
    "print(gender_features('Michael'))\n",
    "print(gender_features('Michelle'))\n",
    "print(gender_features('Francis'))\n",
    "print(gender_features('Frances'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it out with a Naive Bayes classifier. First load some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import names\n",
    "\n",
    "labeled_names = (\n",
    "    [(name, 'male') for name in names.words('male.txt')]\n",
    "    + [(name, 'female') for name in names.words('female.txt')]\n",
    ")\n",
    "# random.shuffle(labeled_names)  # why do this? (hint: take a look at the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create training and test splits (set aside 10% of the data for testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0  # TODO: find the index that splits 1/10 of the data\n",
    "test_set = []  # TODO: use index and labeled_names to get 1/10 of the data\n",
    "train_set = []  # TODO: use index and labeled_names to get 9/10 of the data\n",
    "print('test', len(test_set),\n",
    "      'male:', sum(1 for _, label in test_set if label == 'male'),\n",
    "      'female:', sum(1 for _, label in test_set if label == 'female'))\n",
    "print('train', len(train_set),\n",
    "      'male:', sum(1 for pair in train_set if pair[1] == 'male'),\n",
    "      'female:', sum(1 for _, label in train_set if label == 'female'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: inspect the train set, try out gender_features() on an instance, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []  # TODO: compute gender_features for each (name, label) in train_set, pair result with label\n",
    "test = []  # TODO: do the same for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)  # Train a NB classifier with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.classify(gender_features('Michael')))\n",
    "print(classifier.classify(gender_features('Michelle')))\n",
    "print(classifier.classify(gender_features('Francis')))\n",
    "print(classifier.classify(gender_features('Frances')))\n",
    "print()\n",
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender identification for Japanese\n",
    "\n",
    "Now do the same for Japanese names. The ENAMDICT file contains over 100,000 Japanese names, annotated as (m) for male and (f) for female (and some others for unspecied, family names, etc., which we will ignore). First download the file from here: http://compling.hss.ntu.edu.sg/courses/hg2051/code/enamdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "raw_data = request.urlopen('http://compling.hss.ntu.edu.sg/courses/hg2051/code/enamdict').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And decode from UTF-8. We will need to parse this file (probably with regular expressions) in order to generate (name, label) pairs. So after decoding, inspect the data to get an idea of how to parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.decode('utf-8')\n",
    "print(data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name in kanji is the first letter(s) of each line followed by the transliteration in hiragana inside `[` and `]` characters, then the transliteration in romaji inside `/` and `/`. Inside `/` and `/` is also the gender label inside of parentheses. Assuming we want to model our features on the kanji name, we can ignore all the transliteration data. So we need to first capture the name (`r'^(\\w+) \\['`), followed by anything (`.*`) until we see the label (`\\(([mf])\\)`). Since we only match on `m` and `f`, we ignore any names with other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "jpn_data = []\n",
    "for line in data.splitlines():\n",
    "    m = None  # TODO: write a regular expression to capture the name and gender\n",
    "    if m:\n",
    "        jpn_data.append(m.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to process the data for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(jpn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(len(jpn_data) / 10)\n",
    "j_train_set = jpn_data[index:]\n",
    "j_test_set = jpn_data[:index]\n",
    "print('train', len(j_train_set),\n",
    "      'm:', sum(1 for pair in j_train_set if pair[1] == 'm'),\n",
    "      'f:', sum(1 for _, label in j_train_set if label == 'f'))\n",
    "print('test', len(j_test_set),\n",
    "      'm:', sum(1 for _, label in j_test_set if label == 'm'),\n",
    "      'f:', sum(1 for _, label in j_test_set if label == 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gender_features()` may be relevant for Japanese data as well, as long as it doesn't have any English-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_train = []  # TODO: compute gender_features() for j_train_set, pair each with label\n",
    "\n",
    "j_test = []  # TODO: do the same for j_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_features('太郎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_classifier = nltk.NaiveBayesClassifier.train(j_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('太郎', j_classifier.classify(gender_features('太郎')))\n",
    "print('文美', j_classifier.classify(gender_features('文美')))\n",
    "print('香月', j_classifier.classify(gender_features('香月')))\n",
    "print('良男', j_classifier.classify(gender_features('良男')))\n",
    "print('恵里香', j_classifier.classify(gender_features('恵里香')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(j_classifier, j_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Not all features are useful, so the task of feature selection tries to choose the best ones. There are many methods for selecting relevant features, but for now let's just see what the model thinks are the most informative ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In order to evaluate supervised ML systems, we need gold test data. It is imperative that you do not evaluate your system on the data that you trained it on, as the evaluation will be meaningless at best and often deceptive. The first task is to split your data into separate sets for training and evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits\n",
    "\n",
    "A normal proportion of test data to the rest is 10:90, and the remainder often sets another 10% aside as development data, with the final remainder as trainign data:\n",
    "\n",
    "<table>\n",
    "    <tr><td style=\"background-color: #1FC3AA; width: 400px; font-size: 16pt; color: black;\">Training:<br/> 80%</td>\n",
    "        <td style=\"background-color: #84ded0; width: 50px; font-size: 16pt; color: black;\">Dev:<br/> 10%</td>\n",
    "        <td style=\"background-color: #8624F5; width: 50px; font-size: 16pt; color: white;\">Test:<br/> 10%</td></tr>\n",
    "</table>\n",
    "\n",
    "Note that the terminology for these splits is sometimes inconsistent.\n",
    "\n",
    "* **training** : used to learn a model; standard\n",
    "* **development** : used to evaluate a model for refinement or tuning\n",
    "  * sometimes called **validation** or **tuning** data\n",
    "  * sometimes refers to **training** + **validation**\n",
    "* **test** : used for final evaluation of a model; standard\n",
    "  * sometimes called **evaluation** or **holdout** data\n",
    "\n",
    "See [this Wikipedia article](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) or [this Google ML course](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data) for more info.\n",
    "\n",
    "How you split the data is important. Simply taking contiguous blocks or shuffling the data can lead to unrepresentative samples. Often you need to look at the data first in order to decide how to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall, and F-score\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td colspan=\"2\" style=\"text-align: center\">Gold Values</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">System Output</td>\n",
    "        <td>\n",
    "            <table style=\"border: 1px solid black\">\n",
    "                <tr><td>True Positive</td><td>False Positive</td></tr>\n",
    "                <tr><td>False Negative</td><td>True Negative</td></tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Define the following terms:\n",
    "\n",
    "* Accuracy: `acc = (TP + TN) / (TP + FP + FN + TN)`\n",
    "* Precision: `P = TP / (TP + FP)`\n",
    "* Recall: `R = TP / (TP + FN)`\n",
    "* F-score: `F = (2 * P * R) / (P + R)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are a machine learning model that learns individual decisions that best split the data into separate categories. Entropy and Information-gain are metrics used to determine how well a feature splits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is defined as\n",
    "\n",
    "$ - \\sum_x{p(x) * log_2{p(x)}} $\n",
    "\n",
    "For $p(x)$ we can use the `freq()` method of `nltk.FreqDist` which returns the frequency of some item as the proportion of the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist('mmfff')\n",
    "fd.freq('m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given some list of labels (extracted from the gold instances), we can calculate the entropy of that list as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    fd = nltk.FreqDist(labels)\n",
    "    # TODO: calculate and return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(['m', 'm', 'f', 'f', 'f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information gain is defined for this task as the difference in the entropy of a current sent of data and the set resulting by splitting it on some feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a DecisionTreeClassifier\n",
    "\n",
    "Now try to train a `nltk.DecisionTreeClassifier`. You can use the same data as before with the feature dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = nltk.DecisionTreeClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(dt_classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_dt_classifier = nltk.DecisionTreeClassifier.train(j_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(j_dt_classifier, j_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy did not change for the English data but it went up a bit for the Japanese data. Different models (Naive Bayes, Decision Tree, etc.) have different characteristics and may do better on certain kinds of data than others. Sometimes you can just try out a few to find one that works well, but sometimes the cost (time, money) of training a new model prevents you from exploring all options, so it is good to develop some intuition about the kinds of data that each does well on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}