<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>HG2051 – Week 11</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="css/main.css?v=1" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Week 11</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#reading">Reading</a></li>
<li><a href="#additional-reading">Additional Reading</a></li>
<li><a href="#testing-your-knowledge">Testing Your Knowledge</a><ul>
<li><a href="#questions">Questions</a></li>
<li><a href="#practical-work">Practical Work</a></li>
</ul></li>
</ul>
</nav>
<main>
<h2 id="learning-objectives">Learning Objectives</h2>
<ul>
<li>Concepts: <span class="eng">machine learning</span> <span class="eng">supervised learning</span> <span class="eng">unsupervised learning</span> <span class="eng">classification</span> <span class="eng">evaluation</span> <span class="eng">decision trees</span> <span class="eng">entropy</span> <span class="nlp">linguistic features</span></li>
</ul>
<p>(color key: <span class="py">Python/Programming</span> <span class="nlp">NLP/CL</span> <span class="eng">Software Engineering</span>)</p>
<h2 id="reading">Reading</h2>
<ul>
<li><a href="http://www.nltk.org/book/ch06.html">NLTK 6 – Learning to Classify Text</a>
<ul>
<li><a href="http://www.nltk.org/book/ch06.html#supervised-classification">NLTK 6.1 – Supervised Classification</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#evaluation">NLTK 6.3 – Evaluation</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#decision-trees">NLTK 6.4 – Decision Trees</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#modeling-linguistic-patterns">NLTK 6.7 – Modeling Linguistic Patterns</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#summary">NLTK 6.8 – Summary</a></li>
</ul></li>
</ul>
<p>This week covers an introduction to <strong>machine learning</strong>. Last week we looked at part-of-speech taggers that were trained by looking at examples of gold-standard pos-tagged words, and these were a kind of very basic <strong>supervised classification</strong> based on <strong>statistical inference</strong>. Machine learning also uses statistical inference, but often it builds multi-dimensional models using a variety of <strong>features</strong> of data instances instead of just counting them. For example, instead of just counting to find the most frequent part-of-speech tag for some word, we could consider additional features such as whether the word is capitalized, whether it appears after the word “to”, or the domain of the sentence (e.g., “news”, if it is known). Using all of these features as conditions would make a conditional frequency distribution too sparse to be useful (the features/conditions are too discriminating to get a model that generalizes to unseen data), so instead these features are used in a way that does not <strong>overfit</strong> to the training data.</p>
<h2 id="additional-reading">Additional Reading</h2>
<ul>
<li><a href="http://www.nltk.org/book/ch06.html#further-examples-of-supervised-classification">NLTK 6.2 – Further Examples of Supervised Classification</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#naive-bayes-classifiers">NLTK 6.5 – Naive Bayes Classifiers</a></li>
<li><a href="http://www.nltk.org/book/ch06.html#maximum-entropy-classifiers">NLTK 6.6 – Maximum Entropy Classifiers</a></li>
</ul>
<h2 id="testing-your-knowledge">Testing Your Knowledge</h2>
<h3 id="questions">Questions</h3>
<ul>
<li><p><strong>Q:</strong> What does training data need to be used for supervised classification?</p></li>
<li><p><strong>Q:</strong> What are <em>features</em> in a machine learning model?</p></li>
<li><p><strong>Q:</strong> Why is <em>overfitting</em> a problem?</p></li>
<li><p><strong>Q:</strong> How is a <em>development set</em> different from a <em>test set</em>?</p></li>
</ul>
<h3 id="practical-work">Practical Work</h3>
<ul>
<li><p>Try to split the Brown corpus into train, dev, and test sets, but try to make representative of the corpus. Think about features you might use to split the instances (e.g., the category, length of sentence, perplexity, etc.)</p></li>
<li><p>Build a part-of-speech tagger using a Decision Tree, Naive Bayes, or Maximum Entropy classifier (pick one, or all three). Design your features, train your model, evaluate.</p></li>
</ul>
</main>
</body>
</html>
